{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0946f78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlo Alberto\\AppData\\Local\\Temp\\ipykernel_2348\\2618374505.py:147: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filename)\n",
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.91007 | val_0_logloss: 1.07119 |  0:00:09s\n",
      "epoch 1  | loss: 0.6326  | val_0_logloss: 0.96487 |  0:00:17s\n",
      "epoch 2  | loss: 0.62639 | val_0_logloss: 0.78798 |  0:00:25s\n",
      "epoch 3  | loss: 0.62288 | val_0_logloss: 0.81573 |  0:00:32s\n",
      "epoch 4  | loss: 0.62072 | val_0_logloss: 0.7553  |  0:00:40s\n",
      "epoch 5  | loss: 0.61788 | val_0_logloss: 0.74457 |  0:00:47s\n",
      "epoch 6  | loss: 0.616   | val_0_logloss: 0.72267 |  0:00:55s\n",
      "epoch 7  | loss: 0.61525 | val_0_logloss: 0.68948 |  0:01:03s\n",
      "epoch 8  | loss: 0.615   | val_0_logloss: 0.64758 |  0:01:10s\n",
      "epoch 9  | loss: 0.61309 | val_0_logloss: 0.65777 |  0:01:18s\n",
      "epoch 10 | loss: 0.61259 | val_0_logloss: 0.64992 |  0:01:26s\n",
      "epoch 11 | loss: 0.6117  | val_0_logloss: 0.62041 |  0:01:34s\n",
      "epoch 12 | loss: 0.61657 | val_0_logloss: 0.60872 |  0:01:42s\n",
      "epoch 13 | loss: 0.61587 | val_0_logloss: 0.61207 |  0:01:49s\n",
      "epoch 14 | loss: 0.6134  | val_0_logloss: 0.59547 |  0:01:57s\n",
      "epoch 15 | loss: 0.61207 | val_0_logloss: 0.63081 |  0:02:05s\n",
      "epoch 16 | loss: 0.61238 | val_0_logloss: 0.58341 |  0:02:13s\n",
      "epoch 17 | loss: 0.61114 | val_0_logloss: 0.60629 |  0:02:21s\n",
      "epoch 18 | loss: 0.60958 | val_0_logloss: 0.61163 |  0:02:28s\n",
      "epoch 19 | loss: 0.60916 | val_0_logloss: 0.60659 |  0:02:36s\n",
      "epoch 20 | loss: 0.60905 | val_0_logloss: 0.6132  |  0:02:44s\n",
      "epoch 21 | loss: 0.60847 | val_0_logloss: 0.62358 |  0:02:52s\n",
      "epoch 22 | loss: 0.60807 | val_0_logloss: 0.58725 |  0:03:00s\n",
      "epoch 23 | loss: 0.60766 | val_0_logloss: 0.62361 |  0:03:08s\n",
      "epoch 24 | loss: 0.60693 | val_0_logloss: 0.60942 |  0:03:16s\n",
      "epoch 25 | loss: 0.60791 | val_0_logloss: 0.61421 |  0:03:24s\n",
      "epoch 26 | loss: 0.60683 | val_0_logloss: 0.59162 |  0:03:32s\n",
      "epoch 27 | loss: 0.60716 | val_0_logloss: 0.61336 |  0:03:39s\n",
      "epoch 28 | loss: 0.60592 | val_0_logloss: 0.59324 |  0:03:47s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 16 and best_val_0_logloss = 0.58341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6750823230333529\n",
      "Test accuracy: 0.6736216931413578\n",
      "Test precision: 0.334475528548409\n",
      "Test recall: 0.64151400059577\n",
      "Test F1-score: 0.43969884514770624\n",
      "Test AUC-ROC score: 0.7233147377384213\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.metrics import LogLoss\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys\n",
    "\n",
    "class AUPRC(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"auprc\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        #print(y_true[1:10])\n",
    "        #print(y_pred[1:10])\n",
    "        #sys.exit()\n",
    "        \n",
    "        return average_precision_score(y_true, y_pred[ :,1])\n",
    "\n",
    "class AUC(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"auc\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        #print(y_true[1:10])\n",
    "        #print(y_pred[1:10])\n",
    "        #sys.exit()\n",
    "        \n",
    "        return roc_auc_score(y_true, y_pred[ :,1])\n",
    "    \n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"precision\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        y_pred_class = y_pred.argmax(axis=1)\n",
    "        return precision_score(y_true, y_pred_class)\n",
    "    \n",
    "class Recall(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"recall\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        y_pred_class = y_pred.argmax(axis=1)\n",
    "        return recall_score(y_true, y_pred_class)\n",
    "\n",
    "class AsymmetricHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, false_positive_weight=0.5, false_negative_weight=1):\n",
    "        super(AsymmetricHuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        self.false_positive_weight = false_positive_weight\n",
    "        self.false_negative_weight = false_negative_weight\n",
    "\n",
    "    def forward(self, y_pred_logits, y_true):\n",
    "        y_pred = torch.nn.functional.softmax(y_pred_logits, dim=1)  # Apply softmax activation to convert logits to probabilities\n",
    "        y_true_one_hot = torch.nn.functional.one_hot(y_true, num_classes=y_pred_logits.shape[1])  # Convert y_true to one-hot encoding\n",
    "        y_true_one_hot = y_true_one_hot.type_as(y_pred)  # Convert to the same dtype as y_pred\n",
    "        error = y_true_one_hot - y_pred\n",
    "        abs_error = torch.abs(error)\n",
    "\n",
    "        # Different weights for false positive and false negative errors\n",
    "        weight = torch.where(error > 0, self.false_positive_weight, self.false_negative_weight)\n",
    "\n",
    "        # Calculate the asymmetric Huber loss\n",
    "        huber_loss = torch.where(abs_error <= self.delta,\n",
    "                                 0.5 * weight * (error ** 2),\n",
    "                                 weight * self.delta * (abs_error - 0.5 * self.delta))\n",
    "        return torch.mean(huber_loss)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate the cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "        # Get the probability of the true class\n",
    "        prob_true_class = torch.gather(F.softmax(inputs, dim=1), 1, targets.view(-1, 1))\n",
    "\n",
    "        # Calculate the modulating factor\n",
    "        modulating_factor = (1 - prob_true_class) ** self.gamma\n",
    "\n",
    "        # Calculate the class weights based on the targets\n",
    "        class_weights = torch.where(targets.view(-1, 1) == 1, self.alpha, 1 - self.alpha)\n",
    "\n",
    "        # Calculate the focal loss\n",
    "        focal_loss = class_weights * modulating_factor * ce_loss\n",
    "\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def visualize_masks(model, x):\n",
    "    \"\"\"\n",
    "    Visualize the masks for a specific example.\n",
    "\n",
    "    Parameters:\n",
    "    model: The trained TabNet model.\n",
    "    x: A single example from the input features (X).\n",
    "    \"\"\"\n",
    "    # Run the model in explain mode to get the masks\n",
    "    M_explain, _ = model.explain(torch.from_numpy(x.astype(np.float32)).unsqueeze(0))\n",
    "\n",
    "    # Extract the masks\n",
    "    masks = M_explain.squeeze()\n",
    "\n",
    "    # Ensure the masks array has the correct shape\n",
    "    if masks.ndim == 1:\n",
    "        masks = masks[:, np.newaxis]\n",
    "\n",
    "    masks = masks.T\n",
    "\n",
    "    # Plot the masks as a heatmap\n",
    "    plt.figure(figsize=(10, model.n_steps))\n",
    "    sns.heatmap(masks, cmap='viridis', linewidths=0.1, linecolor='white', cbar=False, yticklabels=False)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('TabNet Mask Visualization')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def preprocess_data(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    def separate_features(dataframe):\n",
    "        categorical_features = []\n",
    "        continuous_features = []\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if dataframe[column].dtype == 'object' or dataframe[column].dtype.name == 'category':\n",
    "                categorical_features.append(column)\n",
    "            elif dataframe[column].dtype == 'int64' or dataframe[column].dtype == 'float64':\n",
    "                continuous_features.append(column)\n",
    "            else:\n",
    "                print(f'Unhandled data type in column \"{column}\": {dataframe[column].dtype}')\n",
    "\n",
    "        return categorical_features, continuous_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data = data[data['loan_status'].isin(['Fully Paid','Charged Off','Defaulted'])]\n",
    "    leakage_features = [\n",
    "        'out_prncp',\n",
    "        'out_prncp_inv',\n",
    "        'total_pymnt',\n",
    "        'total_pymnt_inv',\n",
    "        'total_rec_prncp',\n",
    "        'total_rec_int',\n",
    "        'total_rec_late_fee',\n",
    "        'recoveries',\n",
    "        'collection_recovery_fee',\n",
    "        'last_pymnt_d',\n",
    "        'last_pymnt_amnt',\n",
    "        'next_pymnt_d',\n",
    "        'last_credit_pull_d',\n",
    "        'debt_settlement_flag',\n",
    "        'debt_settlement_flag_date',\n",
    "        'settlement_status',\n",
    "        'settlement_date',\n",
    "        'settlement_amount',\n",
    "        'settlement_percentage',\n",
    "        'settlement_term',\n",
    "        'last_fico_range_high',\n",
    "        'last_fico_range_low',\n",
    "        'id',\n",
    "        'url',\n",
    "        'emp_title',\n",
    "        'title'\n",
    "    ]\n",
    "\n",
    "    # Drop the leakage features\n",
    "    data = data.drop(leakage_features, axis=1)\n",
    "\n",
    "    # Calculate the percentage of missing values for each feature\n",
    "    missing_values = data.isnull().sum() / len(data) * 100\n",
    "\n",
    "    # Identify columns with more than 90% missing values\n",
    "    columns_to_drop = missing_values[missing_values > 90].index\n",
    "\n",
    "    # Drop the identified columns\n",
    "    data = data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    categorical_features, continuous_features = separate_features(data)\n",
    "    categorical_features.remove('loan_status')\n",
    "\n",
    "   \n",
    "    data[continuous_features] = data[continuous_features].fillna(-1)\n",
    "    \n",
    "    \n",
    "    # Fill categorical NAs with \"missing\" and convert all elements to strings\n",
    "    data[categorical_features] = data[categorical_features].fillna('missing').astype(str)\n",
    "\n",
    "    data.isna().sum().sum()\n",
    "    \n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        label_encoder = LabelEncoder()\n",
    "        data[feature] = label_encoder.fit_transform(data[feature])\n",
    "    \n",
    "\n",
    "\n",
    "    # Preprocess loan status labels with custom encoding\n",
    "    custom_encoding = {\n",
    "        'Fully Paid': 0,\n",
    "        'Charged Off': 1,\n",
    "        'Default': 1,\n",
    "    \n",
    "    }\n",
    "    data['loan_status'] = data['loan_status'].replace(custom_encoding)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#a=torch.cuda.FloatTensor()\n",
    "display(torch.cuda.is_available())\n",
    "#sys.exit()\n",
    "\n",
    "filename = 'accepted_2007_to_2018Q4.csv'\n",
    "df = preprocess_data(filename)\n",
    "X = df.drop(columns=['loan_status']).values\n",
    "y = df['loan_status'].values\n",
    "\n",
    "\n",
    "pretrainer_model_path = 'tabnet_pretrainer.pth.zip'\n",
    "\n",
    "\n",
    "# Instantiate the TabNetPretrainer\n",
    "pretrainer = TabNetPretrainer()\n",
    "pretrainer.load_model(pretrainer_model_path) \n",
    "\n",
    "\n",
    "\n",
    "#Split the dataset\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "#Undersample the training set\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Adjust hyperparameters according to the TabNet paper\n",
    "tabnet_params = dict(\n",
    "    n_d=64,\n",
    "    n_a=64,\n",
    "    n_steps=5,\n",
    "    gamma=1.6,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "    mask_type='entmax',\n",
    "    scheduler_params=dict(T_0=200, T_mult=1, eta_min=1e-6, last_epoch=-1, verbose=False),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    seed=42,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "#loss_function = FocalLoss()\n",
    "\n",
    "# Instantiate the TabNetClassifier with the pretrained weights\n",
    "tabnet_model = TabNetClassifier(**tabnet_params)\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_train_resampled,\n",
    "    y_train_resampled,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['logloss'],\n",
    "    max_epochs=300,\n",
    "    batch_size=16384,\n",
    "    virtual_batch_size=512,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    from_unsupervised=pretrainer,\n",
    "    patience=12,\n",
    "    #loss_fn=loss_function\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = tabnet_model.predict(X_test)\n",
    "y_test_pred_proba = tabnet_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_val_pred = tabnet_model.predict(X_val)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "val_accuracy =accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy =accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1_score = f1_score(y_test, y_test_pred)\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print(f\"Validation accuracy: {val_accuracy}\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test precision: {test_precision}\")\n",
    "print(f\"Test recall: {test_recall}\")\n",
    "print(f\"Test F1-score: {test_f1_score}\")\n",
    "print(f\"Test AUC-ROC score: {test_auc_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c799ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_model.pth.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tabnet_model.pth.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model_path = 'tabnet_model.pth'\n",
    "tabnet_model.save_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e82448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
